Index,Question
1,What actually happens in the self-attention layer of each Encoding layer?
2,"What is the key difference between Dense, LSTM and CNN for text analysis?"
3,Any idea which is the largest NLP implementation in the Internet?
4,how do we know which type of word embedding to use?
5,How would NLP interpret metaphors?
6,We used 3 different techniques for the embedding problem. DO we just check all 3 and see which gives the best result?
7,How could we use NLP to do location inferencing?
8,Does emoji recognition fall in the scope of NLP or CNN?
9,When do we flatten layer
10,Could you go through the steps of an Encoder in Transformer? Eg. How each output is passed to the last encoder layer
11,When to use LSTM for text analysis?
12,can we have the lecture notes and code for next summarization?
13,"With Elmo, Bert and other complicated models, is there a reason (besides training time) to utilize a pure LSTM/RNN approach?"
14,Does NLP have greater complexity/require more resources for similar results than other types of machine learning?
15,Is there any limitation in NLP?
16,What features can be used for accuracy improvement of a classification model
17,Traditional NLP techniques have mainly evolved around English Language (i.e POS Tagging). Will machine learning enable us to move towards techniques that are more language independent?
18,How could I solve an NLP problem from scratch?
19,Can NLP decode short-forms?
20,No question
